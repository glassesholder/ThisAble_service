import os
import streamlit as st
from streamlit_pills import pills

from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate


#ê¸°ë³¸ì ì¸ chatbot uië¥¼ ìœ„í•œ style ì‘ì„±
def CPT():
    st.markdown(
    """
    <style>
    body {
        background-color: #D2E0FB; /* ì—°í•œ í•˜ëŠ˜ìƒ‰ ë°°ê²½ */
    }
    /* ì „ì²´ ì±—ë´‡ ì°½ ê°€ìš´ë° ì •ë ¬ */
    .full-container {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
    }
    /* ì±—ë´‡ ì°½ ìŠ¤íƒ€ì¼ë§ */
    .chat-container {
        width: 90%; /* ë„ˆë¹„ë¥¼ ì¡°ì •í•˜ì—¬ ëŒ€í™”ì°½ì„ ë„“ê²Œ ì„¤ì •í•©ë‹ˆë‹¤ */
        padding: 20px;
        background-color: #F9F3CC;
        border-radius: 10px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        margin-bottom: 30px;
    }

    .chat-container2 {
        width: 90%; /* ë„ˆë¹„ë¥¼ ì¡°ì •í•˜ì—¬ ëŒ€í™”ì°½ì„ ë„“ê²Œ ì„¤ì •í•©ë‹ˆë‹¤ */
        padding: 20px;
        background-color: #D7E5CA;
        border-radius: 10px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        float: right;
    }
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ë§ */
    .user-msg {
        background-color: #D7E5CA;
        color: #333;
        border-radius: 10px;
        padding: 10px 15px;
        margin-bottom: 10px;
    }
    
    /* ì±—ë´‡ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ë§ */
    .assistant-msg {
        background-color: #F9F3CC;
        color: black;
        border-radius: 10px;
        padding: 10px 15px;
        margin-bottom: 10px;
    }
    </style>
    """,
    unsafe_allow_html=True
    )

    os.environ["OPENAI_API_KEY"] = os.getenv("OPEN_API_KEY")

    # PDF íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    loader = PyPDFLoader('./files/ë¯¿ìŒ_í•™ìŠµì§€_êµì‚¬ìš©.pdf')
    documents = loader.load()

    # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë‚˜ëˆ„ê¸°
    text_splitter = CharacterTextSplitter(chunk_size=1, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)

    # ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•œ ë’¤, vector_storeì— ì €ì¥
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(texts, embeddings)
    retriever = vector_store.as_retriever(search_kwargs={"k": 1})

    # ì‚¬ê³ ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•œ ì±—ë´‡ system_prompt ì„¤ì •
    system_template_hint = """ë‹¹ì‹ ì€ ì¤‘ë“± íŠ¹ìˆ˜êµìœ¡ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìëŠ” ì¤‘í•™êµ ë˜ëŠ” ê³ ë“±í•™êµì˜ íŠ¹ìˆ˜êµìœ¡ ëŒ€ìƒìì´ë©°, ì •ì‹  ì—°ë ¹ì€ 9~12ì„¸ ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì•¼ í•˜ë©°, ì‰¬ìš´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ë‹µë³€ì— ë¶€ì •ì ì¸ ë‹¨ì–´ë¥¼ í¬í•¨í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
    
    ----------------
    {summaries}
    You MUST answer in Korean and in Markdown format:"""
    messages_hint = [
        SystemMessagePromptTemplate.from_template(system_template_hint),
        HumanMessagePromptTemplate.from_template("{question}")
    ]

    prompt_hint = ChatPromptTemplate.from_messages(messages_hint)
    chain_type_kwargs_hint = {"prompt": prompt_hint}
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    #ì‚¬ê³ ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•œ ì±—ë´‡ ìƒì„±
    chain_hint = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs=chain_type_kwargs_hint
    )

    def generate_response_hint(input_text):
        result = chain_hint(input_text)
        return result['answer']


    st.caption('ì±—ë´‡ì€ GPT-3.5-Turboì— RAG ê¸°ìˆ ì„ ì ìš©í•˜ì—¬ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.')
    st.divider()
    st.header(f'ë¯¿ìŒë‹˜, ë°˜ê°€ì›Œìš”:wave:')
    # st.markdown(":red[íŒŒì´ì¬ìœ¼ë¡œ 00í•˜ëŠ” ë°©ë²•ì´ ê¶ê¸ˆí•´.] ë˜ëŠ” :red[00í•˜ëŠ” ì½”ë“œë¥¼ ë§Œë“¤ê³  ì‹¶ì–´.]ì™€ ê°™ì´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!")
    
    # cptë´‡ì´ ë§í•´ì£¼ëŠ” ì²« ë¬¸ì¥ ìƒì„±
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ë¯¿ìŒë‹˜ì„ ìœ„í•œ ì±—ë´‡ì´ì—ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

    
    # cptë´‡ì´ ì‘ë‹µí•œ ì§ì „ ì‘ë‹µì„ ì €ì¥í•  ê³µê°„
    if "last_question" not in st.session_state:
        st.session_state["last_question"] = ""

    # cptë´‡ê³¼ ë‚˜ëˆˆ ì´ì „ ëŒ€í™” ê°€ì ¸ì™€ì„œ ì±„íŒ…ì°½ì— í‘œì‹œ(ì¦‰, ê·¸ ì „ì— ìˆë˜ ê²ƒ)
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f'<div class="chat-container2"><div class="user-msg">{msg["content"]}</div></div>', unsafe_allow_html=True)
        elif msg["role"] == "assistant":
            if "```" in msg['content']:
                st.chat_message("assistant").write(msg['content'])
            else:
                st.markdown(f'<div class="chat-container"><div class="assistant-msg">{msg["content"]}</div></div>', unsafe_allow_html=True)

    # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê·¸ì— ë”°ë¥¸ ì‘ë‹µ ì¶œë ¥
    if prompt := st.chat_input("ë¯¿ìŒë‹˜ì˜ ê¶ê¸ˆì¦"):  # ë§Œì•½ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì´ ìˆë‹¤ë©´ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="chat-container2"><div class="user-msg">{prompt}</div></div>', unsafe_allow_html=True)
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤ğŸ’¨'):
            msg = generate_response_hint(prompt)

        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.markdown(f'<div class="chat-container"><div class="assistant-msg">{msg}</div></div>', unsafe_allow_html=True)

        st.session_state["last_question"] = msg # ì§ì „ ì§ˆë¬¸ ì €ì¥
        selected = pills("ë‹µë³€ì€ ì–´ë–¤ê°€ìš”?", ["ë§Œì¡±í•´ìš”", "ìŠ¤íƒ€ì¼ì´ ë§ˆìŒì— ì•ˆ ë“¤ì–´ìš”", "ì´í•´ê°€ ì•ˆ ë¼ìš”"], ["ğŸ‘", "ğŸ‘", "â“"], index=False)
